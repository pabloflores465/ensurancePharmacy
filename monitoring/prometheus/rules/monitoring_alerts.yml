# ALERTAS DEL SISTEMA DE MONITOREO
# Monitorea Prometheus, Grafana, Netdata, Alertmanager

groups:
  - name: prometheus_alerts
    interval: 30s
    rules:
      # ALERTA 53: Prometheus Down (auto-monitoreo)
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          service: prometheus
          component: monitoring
        annotations:
          summary: "üî¥ Prometheus CA√çDO - Sistema de monitoreo COMPLETAMENTE CIEGO"
          description: "¬°EMERGENCIA ABSOLUTA! Prometheus est√° ca√≠do. Sin Prometheus: NO HAY MONITOREO de nada, TODAS las alertas detenidas (incluida esta), NO hay m√©tricas, dashboards vac√≠os, visibilidad CERO. Estamos volando completamente a ciegas. No sabremos de problemas hasta que usuarios reporten o sistemas colapsen."
          dashboard: "http://localhost:9090"
          action: "üö® M√ÅXIMA PRIORIDAD: 1) 'docker ps | grep prometheus'. 2) 'docker logs ensurance-prometheus-full --tail 100'. 3) 'docker restart ensurance-prometheus-full'. 4) Verificar espacio en disco (Prometheus falla sin espacio). 5) Revisar config: /monitoring/prometheus/prometheus.yml. 6) RESOLVER INMEDIATAMENTE."
      
      # ALERTA 54: Target Down (cualquier servicio ca√≠do)
      - alert: TargetDown
        expr: up == 0
        for: 2m
        labels:
          severity: warning
          service: monitoring
          component: scraping
        annotations:
          summary: "‚ö†Ô∏è Target {{ $labels.job }} CA√çDO - Sin m√©tricas de {{ $labels.instance }}"
          description: "Prometheus no puede scrapear m√©tricas del target {{ $labels.job }} en {{ $labels.instance }}. Sin m√©tricas de este servicio: no hay alertas espec√≠ficas de √©l, no hay visibilidad de su estado, dashboards sin datos. Posibles causas: servicio ca√≠do, puerto cerrado, firewall bloqueando, configuraci√≥n incorrecta."
          dashboard: "http://localhost:9090/targets"
          action: "üîç Diagnosticar: 1) Ver targets: http://localhost:9090/targets. 2) Verificar servicio: 'docker ps | grep {{ $labels.job }}'. 3) Probar endpoint: 'curl http://{{ $labels.instance }}/metrics'. 4) Ver logs: 'docker logs [container]'. 5) Revisar firewall/networking. 6) Verificar job en prometheus.yml."
      
      # ALERTA 55: Prometheus con alto uso de memoria
      - alert: PrometheusHighMemory
        expr: process_resident_memory_bytes{job="prometheus"} / 1024 / 1024 / 1024 > 2
        for: 5m
        labels:
          severity: warning
          service: prometheus
          component: memory
        annotations:
          summary: "‚ö†Ô∏è Prometheus con memoria alta - {{ $value | humanize }}GB - Riesgo de OOM"
          description: "Prometheus est√° usando {{ $value | humanize }}GB de RAM (umbral: 2GB). Prometheus consume mucha memoria almacenando time series en memoria. Si contin√∫a creciendo: OOM killer puede matarlo, p√©rdida total de monitoreo, datos en memoria perdidos. Causas: demasiadas m√©tricas, alta cardinalidad, retenci√≥n muy larga."
          dashboard: "http://localhost:9090"
          action: "üîç Reducir consumo: 1) Ver uso: http://localhost:9090/tsdb-status. 2) Reducir retenci√≥n: --storage.tsdb.retention.time=7d. 3) Reducir m√©tricas: eliminar jobs innecesarios. 4) Reducir cardinalidad: simplificar labels. 5) Aumentar RAM del contenedor. 6) Implementar remote storage."
      
      # ALERTA 56: Prometheus perdiendo samples
      - alert: PrometheusDroppingSamples
        expr: rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
        for: 2m
        labels:
          severity: warning
          service: prometheus
          component: scraping
        annotations:
          summary: "‚ö†Ô∏è Prometheus descartando {{ $value }} samples/s - P√©rdida de datos"
          description: "Prometheus est√° descartando {{ $value }} samples por segundo porque superan el l√≠mite configurado. Esto significa P√âRDIDA DE DATOS: m√©tricas no se almacenan, alertas pueden no dispararse, dashboards incompletos. Alg√∫n target est√° exportando demasiadas m√©tricas."
          dashboard: "http://localhost:9090/targets"
          action: "üîç Resolver: 1) Identificar target problem√°tico en /targets. 2) Ver m√©tricas: 'curl http://[target]/metrics | wc -l'. 3) Aumentar l√≠mite: sample_limit: 10000 en prometheus.yml. 4) O reducir m√©tricas en el exporter. 5) Considerar si todas las m√©tricas son necesarias."
      
      # ALERTA 57: Prometheus con muchas series activas
      - alert: PrometheusTooManyTimeSeries
        expr: prometheus_tsdb_head_series > 100000
        for: 5m
        labels:
          severity: warning
          service: prometheus
          component: tsdb
        annotations:
          summary: "‚ö†Ô∏è Prometheus con {{ $value }} time series - Cardinalidad alta"
          description: "Prometheus tiene {{ $value }} time series activas (umbral: 100,000). Muchas series causan: alto uso de RAM, queries lentas, scrapes demorados, posible crash por OOM. Generalmente causado por alta cardinalidad de labels (IDs √∫nicos, timestamps, UUIDs en labels)."
          dashboard: "http://localhost:9090/tsdb-status"
          action: "üîç Reducir cardinalidad: 1) Ver series por job: http://localhost:9090/tsdb-status. 2) Identificar labels con alta cardinalidad. 3) Eliminar labels din√°micos (IDs, UUIDs). 4) Usar metric_relabel_configs para drop labels. 5) Reducir scrape de jobs poco importantes. 6) Ejemplo: no usar user_id como label."
      
      # ALERTA 58: Scrape duration alto
      - alert: PrometheusSlowScrapes
        expr: prometheus_target_interval_length_seconds{quantile="0.99"} > 10
        for: 5m
        labels:
          severity: warning
          service: prometheus
          component: scraping
        annotations:
          summary: "‚ö†Ô∏è Scrapes de Prometheus muy lentos - P99: {{ $value }}s"
          description: "El percentil 99 de duraci√≥n de scrapes es {{ $value }} segundos (umbral: 10s). Scrapes lentos causan: m√©tricas desactualizadas, gaps en datos, scrapes perdidos, alertas demoradas. Indica que alg√∫n target tarda mucho en responder /metrics o exporta demasiadas m√©tricas."
          dashboard: "http://localhost:9090/targets"
          action: "üîç Optimizar: 1) Identificar target lento en /targets (columna Last Scrape). 2) Probar manualmente: 'time curl http://[target]/metrics'. 3) Reducir m√©tricas del exporter. 4) Aumentar scrape_interval de ese job. 5) Optimizar exporter si es custom. 6) Paralelizar scrapes si es posible."

  - name: grafana_alerts
    interval: 30s
    rules:
      # ALERTA 59: Grafana Down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          service: grafana
          component: visualization
        annotations:
          summary: "‚ö†Ô∏è Grafana CA√çDO - Dashboards inaccesibles"
          description: "Grafana no responde. Sin Grafana: dashboards no funcionan, visualizaciones no accesibles, usuarios no pueden ver gr√°ficas. Prometheus sigue funcionando y recolectando datos, pero no hay forma visual de verlos. Impacta visibilidad pero no recolecci√≥n de m√©tricas."
          dashboard: "http://localhost:3302"
          action: "üîç Recuperar Grafana: 1) 'docker ps | grep grafana'. 2) 'docker logs ensurance-grafana-full --tail 50'. 3) 'docker restart ensurance-grafana-full'. 4) Grafana tarda ~30s en iniciar. 5) Acceso: http://localhost:3302. 6) Verificar base de datos de Grafana si persiste."

  - name: alertmanager_alerts
    interval: 30s
    rules:
      # ALERTA 61: Alertmanager Down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          service: alertmanager
          component: notifications
        annotations:
          summary: "üî¥ Alertmanager CA√çDO - CERO notificaciones envi√°ndose"
          description: "¬°CR√çTICO! Alertmanager est√° ca√≠do. Sin Alertmanager: NO se env√≠an emails, NO se env√≠an mensajes a Slack, alertas se disparan pero NADIE es notificado. Equipo NO sabr√° de problemas. Es como tener alarma de incendio sin campana. Prometheus sigue alertando pero notificaciones bloqueadas."
          dashboard: "http://localhost:9094"
          action: "üö® URGENTE: 1) 'docker ps | grep alertmanager'. 2) 'docker logs ensurance-alertmanager-full --tail 100'. 3) 'docker restart ensurance-alertmanager-full'. 4) Verificar config: /monitoring/alertmanager/alertmanager.yml. 5) Probar: http://localhost:9094. 6) PRIORIDAD ALTA."
      
      # ALERTA 62: Alertmanager con muchas notificaciones fallidas
      - alert: AlertmanagerFailedNotifications
        expr: rate(alertmanager_notifications_failed_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: alertmanager
          component: notifications
        annotations:
          summary: "‚ö†Ô∏è Alertmanager fallando notificaciones - {{ $value }}/s no se env√≠an"
          description: "Alertmanager est√° intentando enviar notificaciones pero {{ $value }} por segundo est√°n FALLANDO. Alertas se est√°n disparando pero emails/Slack no llegan. Causas comunes: SMTP credentials incorrectos, Slack webhook inv√°lido, red bloqueando, rate limit excedido, timeout de servicios externos."
          dashboard: "http://localhost:9094"
          action: "üîç Depurar notificaciones: 1) Ver logs: 'docker logs ensurance-alertmanager-full | grep -i error'. 2) Verificar Gmail app password. 3) Probar Slack webhook: 'curl -X POST [webhook]'. 4) Revisar alertmanager.yml: receivers config. 5) Verificar conectividad: 'telnet smtp.gmail.com 587'. 6) Revisar rate limits."
      
      # ALERTA 63: Alertmanager con cluster desincronizado (si hay cluster)
      - alert: AlertmanagerClusterUnsynchronized
        expr: alertmanager_cluster_members != on (job) alertmanager_cluster_peers
        for: 5m
        labels:
          severity: warning
          service: alertmanager
          component: cluster
        annotations:
          summary: "‚ö†Ô∏è Cluster Alertmanager desincronizado - Notificaciones duplicadas posibles"
          description: "El cluster de Alertmanager tiene nodos desincronizados. En cluster, todos los nodos deben estar sincronizados para evitar notificaciones duplicadas o perdidas. Desincronizaci√≥n causa: alertas enviadas m√∫ltiples veces, o no enviadas del todo. Solo aplica si tienes m√∫ltiples instancias de Alertmanager."
          dashboard: "http://localhost:9094/#/status"
          action: "üîç Sincronizar cluster: 1) Ver status: http://localhost:9094/#/status. 2) Verificar conectividad entre nodos. 3) Revisar puertos 9094 (API) y 9095 (gossip). 4) Ver logs de cada nodo. 5) Reiniciar nodos desincronizados uno por uno. 6) Si es single-node, ignorar esta alerta."

  - name: portainer_alerts
    interval: 30s
    rules:
      # ALERTA 64: Portainer Down
      - alert: PortainerDown
        expr: up{job="portainer"} == 0
        for: 2m
        labels:
          severity: info
          service: portainer
          component: management
        annotations:
          summary: "‚ÑπÔ∏è Portainer CA√çDO - UI de gesti√≥n Docker no disponible"
          description: "INFO: Portainer no responde. Portainer es una UI web para gestionar contenedores Docker. Sin √©l: no hay interfaz gr√°fica para Docker, pero contenedores siguen funcionando normalmente. Impacto solo en administraci√≥n visual. Se puede gestionar todo por CLI (docker commands)."
          dashboard: "http://localhost:60002"
          action: "üîç Recuperar Portainer: 1) 'docker ps | grep portainer'. 2) 'docker logs ensurance-portainer-full'. 3) 'docker restart ensurance-portainer-full'. 4) Acceso: http://localhost:60002. 5) No es cr√≠tico, solo conveniencia. 6) Usar 'docker ps/logs/restart' mientras tanto."
