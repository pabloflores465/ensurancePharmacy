# ============================================================================
# ALERTAS DE APLICACIONES - NETDATA
# Ensurance Pharmacy Monitoring System
# ============================================================================
# Estas alertas monitorean servicios web, contenedores y aplicaciones
# Las alertas se envían a Prometheus Alertmanager que luego las envía por Email y Slack
# ============================================================================

# ============================================================================
# WEB SERVICE HEALTH CHECKS
# ============================================================================

# Alerta: Servicio web no responde (HTTP check)
# Lógica: Si el servicio web no responde o devuelve error por 1 minuto
alarm: netdata_web_service_down
   on: web_log.response_statuses
 calc: $success
units: requests/s
every: 30s
 crit: $this == 0
delay: up 1m down 3m
 info: El servicio web no está respondiendo exitosamente. \
       Se envía correo y notificación de Slack cuando el servicio no responda.
   to: webmaster

# Alerta: Alta tasa de errores HTTP 5xx
# Lógica: Si más del 5% de las respuestas son errores 5xx por 2 minutos
alarm: netdata_high_http_5xx_errors
   on: web_log.response_statuses
 calc: ($5xx * 100) / ($1xx + $2xx + $3xx + $4xx + $5xx + $other)
units: %
every: 30s
 warn: $this > 5
 crit: $this > 20
delay: up 2m down 5m
 info: Alta tasa de errores HTTP 5xx en el servicio web. \
       WARNING > 5%, CRITICAL > 20%. \
       Se envía correo y notificación de Slack cuando los errores 5xx sobrepasen el 5%.
   to: webmaster

# Alerta: Alta tasa de errores HTTP 4xx
alarm: netdata_high_http_4xx_errors
   on: web_log.response_statuses
 calc: ($4xx * 100) / ($1xx + $2xx + $3xx + $4xx + $5xx + $other)
units: %
every: 30s
 warn: $this > 10
 crit: $this > 30
delay: up 5m down 10m
 info: Alta tasa de errores HTTP 4xx. Posibles problemas con clientes o validación.
   to: webmaster

# Alerta: Tiempo de respuesta web muy alto
alarm: netdata_slow_web_response
   on: web_log.request_processing_time
 calc: $avg
units: ms
every: 30s
 warn: $this > 1000
 crit: $this > 3000
delay: up 3m down 5m
 info: El tiempo de respuesta promedio del servicio web es muy alto. \
       WARNING > 1s, CRITICAL > 3s. \
       Se envía correo y notificación de Slack cuando el tiempo de respuesta sobrepase 1 segundo.
   to: webmaster

# ============================================================================
# DOCKER CONTAINER ALERTS (si Netdata monitorea containers)
# ============================================================================

# Alerta: Contenedor con alto uso de CPU
alarm: netdata_container_high_cpu
   on: cgroup.cpu_limit
 calc: $used
units: %
every: 1m
 warn: $this > 80
 crit: $this > 95
delay: up 3m down 5m
 info: Un contenedor está usando mucho CPU. \
       Se envía correo y notificación de Slack cuando el uso de CPU del contenedor sobrepase el 80%.
   to: sysadmin

# Alerta: Contenedor con alto uso de memoria
alarm: netdata_container_high_memory
   on: cgroup.mem_usage
 calc: ($ram * 100) / $available_ram
units: %
every: 1m
 warn: $this > 85
 crit: $this > 95
delay: up 3m down 5m
 info: Un contenedor está usando mucha memoria. \
       Se envía correo y notificación de Slack cuando el uso de memoria del contenedor sobrepase el 85%.
   to: sysadmin

# Alerta: Contenedor reiniciándose frecuentemente
alarm: netdata_container_restarts
   on: cgroup.throttled
 calc: $throttled_time
units: ms
every: 5m
 warn: $this > 10000
 info: Un contenedor está siendo throttled o reiniciado frecuentemente.
   to: sysadmin

# ============================================================================
# APPLICATION PROCESS ALERTS
# ============================================================================

# Alerta: Proceso Java/Spring Boot con alto uso de memoria
alarm: netdata_java_process_high_memory
   on: apps.mem
 calc: $java
units: MB
every: 1m
 warn: $this > 2048
 crit: $this > 4096
delay: up 5m down 10m
 info: Procesos Java usando memoria excesiva. \
       WARNING > 2GB, CRITICAL > 4GB. \
       Verificar backends backv4 y backv5.
   to: sysadmin

# Alerta: Proceso Java/Spring Boot con alto uso de CPU
alarm: netdata_java_process_high_cpu
   on: apps.cpu
 calc: $java
units: %
every: 1m
 warn: $this > 70
 crit: $this > 90
delay: up 3m down 5m
 info: Procesos Java usando mucho CPU. Verificar backends.
   to: sysadmin

# Alerta: Proceso Node.js/npm con alto uso de memoria
alarm: netdata_nodejs_process_high_memory
   on: apps.mem
 calc: $node
units: MB
every: 1m
 warn: $this > 1024
 crit: $this > 2048
delay: up 5m down 10m
 info: Procesos Node.js usando memoria excesiva. \
       Verificar frontends Vue.js.
   to: sysadmin

# Alerta: Proceso Node.js/npm con alto uso de CPU
alarm: netdata_nodejs_process_high_cpu
   on: apps.cpu
 calc: $node
units: %
every: 1m
 warn: $this > 60
 crit: $this > 80
delay: up 3m down 5m
 info: Procesos Node.js usando mucho CPU. Verificar frontends.
   to: sysadmin

# ============================================================================
# DATABASE / MESSAGE QUEUE PROCESS ALERTS
# ============================================================================

# Alerta: RabbitMQ con alto uso de memoria
alarm: netdata_rabbitmq_high_memory
   on: apps.mem
 calc: $rabbitmq
units: MB
every: 1m
 warn: $this > 1024
 crit: $this > 2048
delay: up 5m down 10m
 info: RabbitMQ usando mucha memoria. \
       WARNING > 1GB, CRITICAL > 2GB. \
       Se envía correo y notificación de Slack cuando RabbitMQ use más de 1GB de memoria.
   to: sysadmin

# Alerta: RabbitMQ con alto uso de CPU
alarm: netdata_rabbitmq_high_cpu
   on: apps.cpu
 calc: $rabbitmq
units: %
every: 1m
 warn: $this > 70
 crit: $this > 90
delay: up 3m down 5m
 info: RabbitMQ usando mucho CPU. \
       Se envía correo y notificación de Slack cuando RabbitMQ use más del 70% de CPU.
   to: sysadmin

# ============================================================================
# PORT/SERVICE AVAILABILITY ALERTS
# ============================================================================

# Alerta: Puerto TCP no escuchando (ejemplo: backend ports)
alarm: netdata_tcp_port_not_listening
   on: netdata.plugin_tc
 calc: $listening
units: ports
every: 1m
 warn: $this == 0
delay: up 2m down 5m
 info: Un puerto TCP esperado no está escuchando. Servicio posiblemente caído.
   to: sysadmin

# ============================================================================
# LOG FILE MONITORING (si está configurado)
# ============================================================================

# Alerta: Muchos errores en logs de aplicación
alarm: netdata_high_application_errors
   on: web_log.response_codes
 calc: $error
units: errors/s
every: 1m
 warn: $this > 5
 crit: $this > 20
delay: up 2m down 5m
 info: Alta tasa de errores en logs de aplicación. \
       Se envía correo y notificación de Slack cuando se detecten más de 5 errores por segundo.
   to: webmaster

# ============================================================================
# NGINX/APACHE WEB SERVER ALERTS
# ============================================================================

# Alerta: Servidor web con muchas conexiones activas
alarm: netdata_web_server_high_connections
   on: web.requests
 calc: $requests
units: requests/s
every: 30s
 warn: $this > 500
 crit: $this > 1000
delay: up 3m down 5m
 info: Servidor web con alta carga de conexiones. \
       WARNING > 500 req/s, CRITICAL > 1000 req/s.
   to: webmaster

# Alerta: Cola de solicitudes web larga
alarm: netdata_web_server_request_queue
   on: web.requests
 calc: $queued
units: requests
every: 30s
 warn: $this > 50
 crit: $this > 100
delay: up 2m down 5m
 info: Cola de solicitudes web muy larga. Posible saturación del servidor.
   to: webmaster
