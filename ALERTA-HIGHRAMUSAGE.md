# ğŸ”” Alerta HighRAMUsage - Primera Alerta del Sistema

## ğŸ“‹ InformaciÃ³n General

- **Nombre:** HighRAMUsage
- **PosiciÃ³n:** #0 (Primera alerta del sistema)
- **Umbral:** RAM > 60%
- **DuraciÃ³n:** 1 minuto
- **Severidad:** Warning
- **Componente:** RAM

## ğŸ¯ Objetivo

Esta alerta se diseÃ±Ã³ para ejecutarse **PRIMERO** en el sistema, con un umbral mÃ¡s bajo (60%) que las otras alertas de memoria, permitiendo:

1. âœ… Detectar uso de RAM tempranamente
2. âœ… Enviar notificaciones antes de que sea crÃ­tico
3. âœ… Dar tiempo para tomar acciÃ³n preventiva
4. âœ… Probar que las notificaciones funcionan correctamente

## ğŸ“§ Canales de NotificaciÃ³n

Esta alerta envÃ­a notificaciones a **AMBOS** canales:

### 1. Gmail (Email)
- **Destinatarios:** 
  - pablopolis2016@gmail.com
  - jflores@unis.edu.gt
- **Asunto:** `âš ï¸ [WARNING] Alerta de Monitoreo - Ensurance Pharmacy`
- **Contenido:** HTML formateado con detalles de la alerta

### 2. Slack
- **Canal:** #ensurance-alerts
- **Formato:** Mensaje estructurado con emojis
- **NotificaciÃ³n:** Desktop + Mobile (segÃºn configuraciÃ³n)

## ğŸ§ª CÃ³mo Probar

### OpciÃ³n 1: Script Interactivo (Recomendado)
```bash
./test-alertas-interactivo.sh
# Selecciona: 1 (Sistema)
# La primera prueba serÃ¡ HighRAMUsage
```

### OpciÃ³n 2: Script Completo
```bash
./test-todas-las-alertas-completo.sh
# La primera alerta en ejecutarse serÃ¡ HighRAMUsage
```

### OpciÃ³n 3: Manual con stress-ng
```bash
# Consumir 65% de RAM por 2 minutos
stress-ng --vm 1 --vm-bytes 65% --timeout 120s

# Esperar 90 segundos
# Verificar alerta en Prometheus: http://localhost:9090
```

## â±ï¸ Tiempo Esperado

1. **Inicio de stress-ng:** 0 segundos
2. **RAM > 60%:** 5-10 segundos
3. **Alerta dispara (for: 1m):** 60 segundos
4. **Alertmanager procesa:** 70 segundos
5. **Email/Slack enviado:** 80-90 segundos

**Total:** ~1.5-2 minutos desde inicio hasta recibir notificaciÃ³n

## ğŸ” Verificar Resultado

### 1. Verificar en Prometheus
```bash
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.labels.alertname=="HighRAMUsage")'
```

### 2. Verificar en Alertmanager
```bash
curl -s http://localhost:9094/api/v1/alerts | jq '.data[] | select(.labels.alertname=="HighRAMUsage")'
```

### 3. Ver logs de envÃ­o
```bash
# Ver logs de Alertmanager
docker logs ensurance-alertmanager-full --tail 50 | grep -i "HighRAMUsage"

# Ver logs de envÃ­o de email
docker logs ensurance-alertmanager-full --tail 50 | grep -i "smtp"

# Ver logs de envÃ­o a Slack
docker logs ensurance-alertmanager-full --tail 50 | grep -i "slack"
```

### 4. Verificar Email
- âœ‰ï¸ Revisa inbox de pablopolis2016@gmail.com
- âœ‰ï¸ Revisa inbox de jflores@unis.edu.gt
- âš ï¸ **IMPORTANTE:** Revisa carpeta de SPAM

### 5. Verificar Slack
- ğŸ’¬ Abre Slack
- ğŸ’¬ Ve al canal #ensurance-alerts
- ğŸ’¬ Busca mensaje con tÃ­tulo "âš ï¸ WARNING - Ensurance Pharmacy"

## ğŸ“Š ExpresiÃ³n de Prometheus

```yaml
expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 60
```

**ExplicaciÃ³n:**
- `node_memory_MemAvailable_bytes`: Memoria disponible (incluye cache/buffers)
- `node_memory_MemTotal_bytes`: Memoria total del sistema
- Calcula porcentaje usado: `(1 - disponible/total) * 100`
- Se dispara cuando > 60%

## ğŸ”§ ConfiguraciÃ³n

### Archivo de Alerta
`/monitoring/prometheus/rules/system_alerts.yml`

```yaml
- name: system_ram_alerts
  interval: 30s
  rules:
    - alert: HighRAMUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 60
      for: 1m
      labels:
        severity: warning
        service: system
        component: ram
      annotations:
        summary: "Alto uso de RAM en {{ $labels.instance }}"
        description: "El uso de RAM estÃ¡ en {{ $value | humanize }}% (umbral: 60%)"
        dashboard: "http://localhost:19999"
        action: "Monitorear consumo de memoria de aplicaciones"
```

### ConfiguraciÃ³n de Alertmanager
`/monitoring/alertmanager/alertmanager.yml`

- **Receiver:** warning-notifications
- **Repeat interval:** 1 hora
- **Group by:** alertname, cluster, service
- **Group wait:** 10 segundos

## ğŸš¨ Si No Llegan Notificaciones

### Problema 1: Email no llega

**DiagnÃ³stico:**
```bash
./test-email-alertmanager.sh
```

**Soluciones:**
1. Verificar App Password de Gmail
2. Revisar carpeta SPAM
3. Ver logs de Alertmanager para errores SMTP
4. Verificar conectividad: `telnet smtp.gmail.com 587`

### Problema 2: Slack no llega

**DiagnÃ³stico:**
```bash
# Ver configuraciÃ³n de Slack
docker exec ensurance-alertmanager-full cat /etc/alertmanager/alertmanager.yml | grep -A 10 "slack_configs"
```

**Soluciones:**
1. Verificar que `SLACK_WEBHOOK_URL_AQUI` estÃ© reemplazado con URL real
2. Verificar que el canal #ensurance-alerts exista
3. Probar webhook manualmente:
   ```bash
   curl -X POST "TU_SLACK_WEBHOOK_URL" \
     -H 'Content-Type: application/json' \
     -d '{"text":"Prueba de webhook"}'
   ```

### Problema 3: Alerta no se dispara

**Verificar:**
```bash
# Ver si Prometheus estÃ¡ scrapeando node-exporter
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="node-exporter")'

# Ver valor actual de memoria
curl -s http://localhost:9090/api/v1/query?query=\(1-\(node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes\)\)*100

# Recargar configuraciÃ³n de Prometheus
curl -X POST http://localhost:9090/-/reload
```

## ğŸ“ˆ Diferencia con Otras Alertas de Memoria

| Alerta | Umbral | For | Severidad | Objetivo |
|--------|---------|-----|-----------|----------|
| **HighRAMUsage** | **60%** | **1m** | **warning** | **DetecciÃ³n temprana** |
| HighMemoryUsage | 80% | 2m | warning | Memoria alta |
| CriticalMemoryUsage | 95% | 1m | critical | Memoria crÃ­tica |

## ğŸ”„ Flujo de Alerta

```
1. Uso RAM > 60%
   â†“
2. Prometheus detecta (cada 30s)
   â†“
3. Espera 1 minuto (for: 1m)
   â†“
4. Si persiste, dispara alerta
   â†“
5. EnvÃ­a a Alertmanager
   â†“
6. Alertmanager agrupa (10s)
   â†“
7. EnvÃ­a a receivers:
   - Gmail (SMTP)
   - Slack (Webhook)
   â†“
8. Usuario recibe notificaciÃ³n
```

## ğŸ“ Notas Importantes

1. âœ… Esta alerta es la **#0** - se ejecuta PRIMERO
2. âœ… Umbral bajo (60%) para detecciÃ³n temprana
3. âœ… EnvÃ­a a Gmail Y Slack simultÃ¡neamente
4. âœ… Se repite cada 1 hora si persiste
5. âš ï¸ Slack requiere configurar webhook URL
6. âš ï¸ Gmail requiere App Password vÃ¡lida

## ğŸš€ Quick Start

Para probar rÃ¡pidamente esta alerta:

```bash
# 1. Ejecutar script interactivo
./test-alertas-interactivo.sh

# 2. Seleccionar opciÃ³n 1 (Sistema)

# 3. Esperar ~2 minutos

# 4. Verificar email y Slack

# 5. Ver logs si no llega
docker logs -f ensurance-alertmanager-full
```

## ğŸ“ Soporte

Si la alerta no funciona:
1. Ejecuta: `./test-email-alertmanager.sh`
2. Revisa logs: `docker logs ensurance-alertmanager-full --tail 100`
3. Verifica Slack webhook estÃ¡ configurado
4. Consulta: GUIA-PRUEBA-CORREOS-ALERTAS.md

---

**Ãšltima actualizaciÃ³n:** 31 de Octubre, 2025
