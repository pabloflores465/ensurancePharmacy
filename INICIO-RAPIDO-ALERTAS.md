# ğŸš€ Inicio RÃ¡pido - Sistema de Alertas

## âœ… Estado del Sistema

### Servicios Activos
- âœ… Prometheus: 25 grupos de reglas cargados (64 alertas)
- âœ… AlertManager: Configurado con Email + Slack
- âœ… Netdata: 4 archivos de health (~45 alertas)
- âœ… Grafana: Dashboards k6 y system-metrics

### Total de Alertas
- **Prometheus/Grafana:** 64 alertas
- **Netdata:** ~45 alertas
- **TOTAL:** ~110 alertas Ãºnicas

---

## ğŸ”§ ConfiguraciÃ³n Inicial (REQUERIDO)

### 1. Configurar Webhook de Slack

```bash
# Obtener webhook de Slack:
# 1. Ir a https://api.slack.com/apps
# 2. Crear app "Ensurance Alerts"
# 3. Activar "Incoming Webhooks"
# 4. AÃ±adir webhook al canal #ensurance-alerts
# 5. Copiar URL del webhook

# Configurar en el sistema:
./configure-slack-webhook.sh "https://hooks.slack.com/services/T.../B.../XXX..."
```

### 2. Reiniciar Servicios

```bash
# Reiniciar AlertManager para aplicar configuraciÃ³n
docker compose -f docker-compose.full.yml restart alertmanager

# Reiniciar Netdata para cargar health alerts
docker compose -f docker-compose.full.yml restart netdata
```

---

## ğŸ§ª Probar Notificaciones

### Test 1: Email (NodeExporter Down - WARNING)
```bash
# Detener servicio
docker stop ensurance-node-exporter-full

# Esperar 2 minutos (alerta se dispara)
sleep 120

# Verificar email en pablopolis2016@gmail.com y jflores@unis.edu.gt

# Restaurar servicio
docker start ensurance-node-exporter-full
```

### Test 2: Slack (RabbitMQ Down - CRITICAL)
```bash
# Detener servicio
docker stop ensurance-rabbitmq-full

# Esperar 90 segundos (alerta se dispara)
sleep 90

# Verificar mensaje en canal #ensurance-alerts con @channel

# Restaurar servicio
docker start ensurance-rabbitmq-full
```

### Verificar Notificaciones Enviadas
```bash
# Ver emails enviados (Ãºltimos 10 minutos)
docker logs ensurance-alertmanager-full --since 10m 2>&1 | grep -i "email.*notify"

# Ver mensajes Slack enviados (Ãºltimos 10 minutos)
docker logs ensurance-alertmanager-full --since 10m 2>&1 | grep -i "slack.*notify"
```

---

## ğŸ“Š Ver Alertas Activas

### Prometheus
```bash
# Browser
open http://localhost:9090/alerts

# CLI
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.state=="firing")'
```

### AlertManager
```bash
# Browser
open http://localhost:9094

# CLI
curl -s http://localhost:9094/api/v2/alerts
```

### Netdata
```bash
# Browser
open http://localhost:19999

# CLI
curl -s http://localhost:19999/api/v1/alarms?active
```

---

## ğŸ” Validar Sistema Completo

```bash
# Ejecutar script de validaciÃ³n
./test-all-monitoring-alerts.sh

# Ver reporte
cat alert-testing-*.log
```

---

## ğŸ“‹ Alertas Principales Configuradas

### Grafana/Prometheus (64 alertas)

#### Sistema (11 alertas)
- **HighCPUUsage:** CPU > 70% â†’ Email + Slack
- **HighMemoryUsage:** Memoria > 80% â†’ Email + Slack
- **HighDiskUsage:** Disco > 75% â†’ Email + Slack
- **HighNetworkReceive/Transmit:** > 100MB/s â†’ Email + Slack
- **HighSystemLoad:** Load > 2x CPUs â†’ Email + Slack

#### K6 Stress Testing (8 alertas)
- **K6HighErrorRate:** Errores > 5% â†’ Email + Slack (@channel)
- **K6HighResponseTimeP95:** P95 > 1s â†’ Email + Slack
- **K6FailedChecks:** Checks fallando â†’ Email + Slack

#### CI/CD (12 alertas)
- **JenkinsDown:** Jenkins caÃ­do â†’ Email + Slack (@channel)
- **JenkinsBuildFailed:** Build fallido â†’ Email + Slack
- **DroneServerDown:** Drone caÃ­do â†’ Email + Slack

### Netdata (~45 alertas)

#### Sistema (15 alertas)
- **netdata_high_cpu_usage:** CPU > 70% â†’ Alertmanager â†’ Email + Slack
- **netdata_high_memory_usage:** Memoria > 80% â†’ Alertmanager â†’ Email + Slack
- **netdata_high_disk_usage:** Disco > 75% â†’ Alertmanager â†’ Email + Slack
- **netdata_high_network_receive/transmit:** > 100MB/s â†’ Alertmanager â†’ Email + Slack

#### Aplicaciones (12 alertas)
- **netdata_container_high_cpu:** Container CPU > 80% â†’ Email + Slack
- **netdata_rabbitmq_high_memory:** RabbitMQ > 1GB â†’ Email + Slack
- **netdata_high_http_5xx_errors:** Errores 5xx > 5% â†’ Email + Slack

---

## ğŸ“§ Formato de Notificaciones

### Email - CRITICAL
```
ğŸ”´ [CRÃTICO] Alerta Urgente - Ensurance Pharmacy

âš ï¸ ALERTA CRÃTICA âš ï¸
Alerta: HighCPUUsage
Severidad: CRÃTICO
DescripciÃ³n: El uso de CPU estÃ¡ en 92%
Resumen: CPU sobrepasa el 70%
Servicio: system
Instancia: node-exporter:9100

âš¡ ACCIÃ“N REQUERIDA INMEDIATAMENTE
```

### Slack - CRITICAL
```
ğŸ”´ ALERTA CRÃTICA - Ensurance Pharmacy
<!channel>

âš ï¸ ALERTA CRÃTICA - ACCIÃ“N INMEDIATA REQUERIDA âš ï¸

Alerta: HighCPUUsage
Severidad: CRÃTICO
DescripciÃ³n: El uso de CPU estÃ¡ en 92%
Dashboard: http://localhost:19999
AcciÃ³n: Revisar procesos con alto consumo de CPU
```

---

## ğŸ”§ Troubleshooting

### Alertas no se disparan
```bash
# Verificar reglas cargadas en Prometheus
curl http://localhost:9090/api/v1/rules | jq '.data.groups | length'

# Verificar targets UP
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.health!="up")'

# Verificar health de Netdata
docker exec ensurance-netdata-full ls -la /etc/netdata/health.d/
```

### Email no llega
```bash
# Ver logs de AlertManager
docker logs ensurance-alertmanager-full --tail 100 | grep -i "email"

# Verificar configuraciÃ³n SMTP
grep "smtp_" monitoring/alertmanager/alertmanager.yml.template
```

### Slack no funciona
```bash
# Ver logs de AlertManager
docker logs ensurance-alertmanager-full --tail 100 | grep -i "slack"

# Probar webhook manualmente
curl -X POST "TU_WEBHOOK_URL" \
  -H 'Content-Type: application/json' \
  -d '{"text":"Test desde Ensurance Pharmacy"}'
```

---

## ğŸ“ Archivos Creados

```
monitoring/
â”œâ”€â”€ alertmanager/
â”‚   â””â”€â”€ alertmanager.yml.template  â† Email + Slack configurado (4 receivers)
â”œâ”€â”€ prometheus/rules/
â”‚   â”œâ”€â”€ system_alerts.yml          â† 11 alertas
â”‚   â”œâ”€â”€ application_alerts.yml     â† 9 alertas
â”‚   â”œâ”€â”€ rabbitmq_alerts.yml        â† 12 alertas
â”‚   â”œâ”€â”€ k6_alerts.yml              â† 8 alertas
â”‚   â”œâ”€â”€ cicd_alerts.yml            â† 12 alertas
â”‚   â””â”€â”€ monitoring_alerts.yml      â† 12 alertas
â””â”€â”€ netdata/health.d/
    â”œâ”€â”€ system_alerts.conf         â† 15 alertas
    â”œâ”€â”€ application_alerts.conf    â† 12 alertas
    â”œâ”€â”€ k6_alerts.conf             â† 7 alertas
    â””â”€â”€ pipeline_alerts.conf       â† 10 alertas

Scripts:
â”œâ”€â”€ test-all-monitoring-alerts.sh  â† ValidaciÃ³n completa
â”œâ”€â”€ configure-slack-webhook.sh     â† Configurar Slack
â””â”€â”€ setup-alertmanager-secrets.sh  â† Generar config

DocumentaciÃ³n:
â”œâ”€â”€ ALERTAS-SISTEMA-COMPLETO.md    â† DocumentaciÃ³n completa
â””â”€â”€ INICIO-RAPIDO-ALERTAS.md       â† Esta guÃ­a
```

---

## âœ… Checklist Final

- [x] AlertManager template con Email y Slack
- [x] 64 alertas de Prometheus configuradas
- [x] 45 alertas de Netdata configuradas
- [x] Script de validaciÃ³n creado
- [ ] **Webhook de Slack configurado (PENDIENTE)**
- [ ] **Pruebas de Email ejecutadas (PENDIENTE)**
- [ ] **Pruebas de Slack ejecutadas (PENDIENTE)**

---

**Siguiente Paso:** Configurar webhook de Slack y ejecutar pruebas

```bash
./configure-slack-webhook.sh "TU_WEBHOOK_URL"
./test-all-monitoring-alerts.sh
```
